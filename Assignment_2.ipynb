{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "\n",
    "In this assignment we will cover topics from the previous 3 lectures. We will cover the following topics:\n",
    "\n",
    "1) Training a simple Linear Model\n",
    "\n",
    "2) Implementing Modules with Backprop functionality\n",
    "\n",
    "3) Implementing Convolution Module on Numpy.\n",
    "\n",
    "4) Implement Dropout/Different Optimizer setups.\n",
    "\n",
    "5) Implementing Pool and Training on CIFAR10?\n",
    "\n",
    "\n",
    "It is crucial to get down to the nitty gritty of the code to implement all of these. No external packages (like caffe,pytorch etc), which directly give functions for these steps, are to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple Linear Model\n",
    "\n",
    "In this section, you will write the code to train a Linear Model. The goal is to classify and input $x_n$ of size $n$ into one of $m$ classes. For this goal, you need to create the following parts:\n",
    "\n",
    "1) ** A weight Matrix $W_{n\\times m}$ **, where the Weights are multipled to the input $X_n$ (Vector of size $n$), to find $m$ scores $S_m$ for the $m$ classes.\n",
    "\n",
    "2) ** The Loss function **: We learnt two Kinds of Loss functions:\n",
    "  *  The Hinge Loss: This loss measures, for each sample, how many times were the wrong classes scored above correct class score - $\\Delta$ ? and by how much? This leads to the formulation:\n",
    "  \n",
    "$$\n",
    "L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)\n",
    "$$\n",
    "\n",
    "where $y_i$ is the correct class, and $s_j$ is the score for the $j$-th class (the $j$-th element of $S_m$)\n",
    "  \n",
    "  * The softmax Loss: By interpreting the scores as unnormalized log probabilities for each class, this loss tries to measure dissatisfaction with the scores in terms of the log probability of the right class:\n",
    "\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}\n",
    "$$\n",
    "\n",
    "where $f_{ y_i }$ is the $i$-th element of the output of $W^T_{n \\times m} . X_m$\n",
    "\n",
    "4) ** Regularization term **: In addition to the loss, you need a Regularization term to lead to a more distributed( in case of $L_2$) or sparse (in case of $L_1$) learning of the weights. For example, and having $L_2$ regularization would imply that your loss has the following additional term:\n",
    "\n",
    "$$\n",
    "R(W) = \\sum_k\\sum_l W_{k,l}^2,\n",
    "$$\n",
    "\n",
    "making the total loss:\n",
    "$$\n",
    "L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\\n",
    "$$\n",
    "\n",
    "3) ** An Optimization Procedure **: This refers to the process which tweaks the weight Matrix $W_{n\\times m}$ to reduce the loss function $L$. In our case, this refers to Mini-batch Gradient Descent algorithm. We adjust the weights $W_{n\\times m}$, based on the gradient of the loss $L$ w.r.t. $W_{n\\times m}$. This leads to:\n",
    "\n",
    "$$\n",
    "W_{t+1} = W_{t} + \\alpha \\frac{\\partial L}{\\partial W},\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Additionally, as we will be doing \"mini-batch\" gradient Descent, instead of finding loss over the whole dataset, we find it only for a small sample of the traning data for each learning step we take. Basically,\n",
    "$$\n",
    "W_{t+1} = W_{t} + \\alpha \\frac{\\partial \\sum^{b}{L_{x_i}}}{\\partial W},\n",
    "$$\n",
    "where, $b$ is the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Train a **Single-Layer Classifier** for the MNIST dataset. Guidelines:\n",
    "* Use a loss of your choice.\n",
    "* Keep a validation split of the trainingset for finding the right value of $\\lambda$ for the regularization, and to check for over fitting.\n",
    "* Finally,evaluate the classification performance on the testset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load The Mnist data:\n",
    "# Download data from http://yann.lecun.com/exdb/mnist/\n",
    "# load the data.\n",
    "\n",
    "# split the data into train, and valid\n",
    "\n",
    "# Now a function, which returns a generator random mini-batch of the input data\n",
    "\n",
    "def get_minibatch_function(training_x, training_y):\n",
    "    \n",
    "    def get_minibatch(training_x=training_x, training_y=training_y):\n",
    "        ## Read generator functions if required.\n",
    "\n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        yield mini_x,mini_y\n",
    "        \n",
    "    return get_minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the class Single Layer Classifier\n",
    "class single_layer_classifier():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        # Give the instance a weight matrix, initialized randomly.\n",
    "        \n",
    "    # Define the forward function\n",
    "    def forward(self, input_x):\n",
    "        \n",
    "        # get the scores\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    # Similarly a backward function\n",
    "    # we define 2 backward functions (as Loss = L1 + L2, grad(Loss) = grad(L1) + grad(L2))\n",
    "    \n",
    "    def backward_from_loss(self, grad_from_loss):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the loss w.r.t. the respective element of weight.\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "        \n",
    "    def backward_from_l2(self):\n",
    "        \n",
    "        # this function returns a matrix of the same size as the weights, \n",
    "        # where each element is the partial derivative of the regularization_term\n",
    "        # w.r.t. the respective element of weight.\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        return grad_matrix\n",
    "    \n",
    "    # BONUS\n",
    "    def grad_checker(input_x, grad_matrix):\n",
    "        \n",
    "        # Guess what to do?\n",
    "        \n",
    "        ## WRITE CODE HERE\n",
    "        \n",
    "        if diff<threshold:\n",
    "            return true\n",
    "        else:\n",
    "            return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need the loss functions,one which calculates the loss, \n",
    "# and one which give the backward gradient\n",
    "# Make any one of the suggested losses\n",
    "\n",
    "def loss_forward(input_y,scores):\n",
    "\n",
    "    ## WRITE CODE HERE    \n",
    "    \n",
    "    return loss\n",
    "def loss_backward(loss):\n",
    "    # This part deals with the gradient from the loss to the weight matrix.\n",
    "    # for example, in case of softmax loss(-log(qc)), this part gives grad(loss) w.r.t. qc\n",
    "\n",
    "    ## WRITE CODE HERE    \n",
    "\n",
    "    return grad_from_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally the trainer:\n",
    "\n",
    "# let it be for t iterations:\n",
    "\n",
    "# make an instance of single_layer_classifier,\n",
    "# get the mini-batch yielder.\n",
    "\n",
    "for iter,input_x, input_y in enumerate(get_minibatch()):\n",
    "    \n",
    "    ## Write code here for each iteration of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the performance on the validation set.\n",
    "# find the top-1 accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now make a trainer function based on the above code, which trains for 't' iteration,\n",
    "# and returns the performance on the validation\n",
    "\n",
    "def trainer(iterations, kwargs):\n",
    "\n",
    "    ## WRITE CODE HERE\n",
    "    \n",
    "    return top_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal lambda and iterations t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train on whole dataset with these values,(from scratch)\n",
    "# report final performance on mnist test set.\n",
    "\n",
    "# Find the best performing class and the worst performing class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backprop\n",
    "\n",
    "Now that you have had some experience with single layer networks, its time to go to more complex architectures. But first we need to completely understand and implement backpropagation.\n",
    "\n",
    "## Backpropagation:\n",
    "\n",
    "Simple put, a way of computing gradients of expressions through recursive application of chain rule. If,\n",
    "$$\n",
    "L = f (g (h (\\textbf{x})))\n",
    "$$\n",
    "then,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial f}{\\partial g} \\times \\frac{\\partial g}{\\partial h} \\times\\frac{\\partial h}{\\partial \\textbf{x}} \n",
    "$$\n",
    "\n",
    "** Look into the class Lecture for more detail **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 : Scalar Backpropagation\n",
    "\n",
    "Evaluate the gradient of the following functions w.r.t. the input.\n",
    "\n",
    "1) $$ f(x,y,z) =  log(\\sigma(\\frac{cos(\\pi \\times \\sigma(x))+sin(\\pi \\times \\sigma(y/2))}{z^2}))$$\n",
    "where $\\sigma$ is the sigmoid function. Find gradient for the following values:\n",
    "  * $(x,y,z)$ =  (1,2,3)\n",
    "  * $(x,y,z)$ =  (3,2,1)\n",
    "  * $(x,y,z)$ =  (12,23,31)\n",
    "  * $(x,y,z)$ =  (32,21,13)\n",
    "\n",
    "2) $$ f(x,y,z) = -tan(z) + exp(4x^2 + 3x + 10) - x^{y^z} $$\n",
    "where $\\exp$ is the exponential function. Find gradient for the following values:\n",
    "  * $(x,y,z)$ =  (-0.1 ,2 ,-3)\n",
    "  * $(x,y,z)$ =  (-3, 0.2,0.5)\n",
    "  * $(x,y,z)$ =  (1.2, -2.3, 3.1)\n",
    "  * $(x,y,z)$ =  (3.2, 2.1, -1.3)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To solve this problem, construct the computational graph (will help understanding the problem)(not part of assignment)\n",
    "# Write each component of the graph as a class, with forward and backward functions.\n",
    "\n",
    "# for eg:\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        \n",
    "    def forward():\n",
    "        # save values useful for backpropagation\n",
    "    def backward():\n",
    "        \n",
    "# CAUTION: Carefully treat the input and output dimension variation. At worst, handle them with if statements.\n",
    "# Similarly create the classes for various sub-parts/elements of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now write func_1_creator, \n",
    "# which constructs the graph(all operators), forward and backward functions.\n",
    "\n",
    "class func1():\n",
    "    def __init__(self):\n",
    "        # construct the graph here, \n",
    "        # assign the instances of function modules to self.var\n",
    "        \n",
    "    def forward(x,y,z):\n",
    "        # Using the graph element's forward functions, get the output. \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward():\n",
    "        # Use the saved outputs of each module, and backward() function calls\n",
    "        \n",
    "        return [grad_x,grad_y,grad_z]\n",
    "    \n",
    "    \n",
    "# Similarly,\n",
    "class func2():\n",
    "    def __init__(self):\n",
    "        # construct the graph here, \n",
    "        # assign the instances of function modules to self.var\n",
    "        \n",
    "    def forward(x,y,z):\n",
    "        # Using the graph element's forward functions, get the output. \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward():\n",
    "        # Use the saved outputs of each module, and backward() function calls\n",
    "        \n",
    "        return [grad_x,grad_y,grad_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Modular Vector Backpropagation\n",
    "\n",
    "* Construct a Linear Layer module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Construct a ReLU module, implementing the forward and backward functions for arbitrary sizes.\n",
    "* Create a 2 layer MLP using the constructed modules.\n",
    "\n",
    "* Modifying the functions built in Question 1 , train this two layer MLP for the same data set (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for Linear Layer (Refer code of pytorch/tensorflow package if required.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class for ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your 2 layer MLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best Class and worst class performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the lecture on Jan 31st.\n",
    "\n",
    "# Implementing Convolution Module on Numpy.\n",
    "\n",
    "* This topic will require you to implement the Convolution operation using Numpy.\n",
    "* You will implement two methods of doing it, an intuitive and an optimised way.\n",
    "* Additional operations like dropout, batch norms. \n",
    "* We will use the created Module for interesting task like Blurring, Bilateral Filtering.\n",
    "* Finally, we create the Backprop for this.\n",
    "* Train a Conv model for the same MNIST dataset. (can be a script based training, instead of having it in jupyter notebook.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
